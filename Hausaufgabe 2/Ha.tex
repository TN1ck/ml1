\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\setlength{\parindent}{0cm}
\usepackage{setspace}
\usepackage{mathpazo}
\usepackage{graphicx}
\usepackage{wasysym} 
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage{verbatim}
\usepackage{microtype}
\usepackage{siunitx}
\usepackage{cleveref}
\usepackage[colorlinks=false, pdfborder={0 0 0}]{hyperref}
\usepackage{paralist}
\usepackage{pst-all}
\usepackage{pstricks}
\usepackage{pst-node}
\usepackage{tikz}
\usepackage{tkz-berge}
\usetikzlibrary{trees,petri,decorations,arrows,automata,shapes,shadows,positioning,plotmarks}
\usepackage[a4paper,
left=3.0cm, right=3.0cm,
top=2.0cm, bottom=2.0cm]{geometry}
\usepackage{fullpage}
\usepackage[german]{babel}
%\usepackage{pst-all}
%\usepackage{pstricks}
\setlength{\unitlength}{1cm}
\newcommand{\N}{\mathbb{N}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\D}{\mathcal{D}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\usepackage{mathpazo}

\author{Tom Nick, Niklas Gebauer, Felix Bohmann}
\title{Machine Learning}
\begin{document}
\begin{center}
\Large{\textsc{Machine Learning 1: Assignment 2}} \\
\end{center}

\begin{tabbing}
Tom Nick \hspace{0.9cm}\= 340528\\
Niklas Gebauer\>  340942 
\end{tabbing}

\subsection*{Exercise 1}
\begin{enumerate}[(a)]
\item 
With the definition $P(x_k \mid \theta)$:
$$P(x \mid \theta) = \begin{cases} \theta &\text{if} x = \text{head}\quad \\ 1 - \theta &\text{if}\quad x = \text{tail} \end{cases}$$
We can state the likelihood function $P(\D \mid \theta)$:
\begin{align*}
	P(\D \mid \theta) &= \prod_{k = i}^{n} P(x_k \mid \theta) \\
	&= \theta^5 \cdot (1 - \theta)^2
\end{align*}
\item
For this task we define:
\begin{align*}
f(x) = P(\D \mid \theta) = \theta^5 \cdot (1 - \theta)^2
\end{align*}
To obtain the maximum likelihood solution $\hat{\theta}$ we can now use the standard procedure to find local minima and maxima in $f$. So we will at first set the first derivative to zero and solve for $\theta$:
\begin{align*}
f'(x) &= 0\\
\Leftrightarrow (\theta^5 \cdot (1 - \theta)^2)' &= 0\\
\Leftrightarrow (\theta^7 - 2\theta^6 + \theta^5)' &= 0\\
\Leftrightarrow 7\theta^6 - 12\theta^5 + 5\theta^4 &= 0\\
\Leftrightarrow \theta^4(7\theta^2-12\theta+5) &= 0\\
\Leftrightarrow \theta &= 0 \vee 7\theta^2-12\theta+5 = 0
\end{align*}
So the first possible solution for $\hat{\theta}$ is $0$. Now we can use the p-q-formula to calculate the other solutions:
\begin{align*}
7\theta^2-12\theta+5 &= 0\\
\Leftrightarrow \theta^2-\frac{12}{7}\theta+\frac{5}{7} &= 0\\
\Leftrightarrow \frac{12}{14} \pm \sqrt{(\frac{12}{14})^2-\frac{5}{7}} &= \theta_1,\theta_2\\
\Leftrightarrow \frac{6}{7} \pm \sqrt{\frac{1}{49}} &= \theta_1,\theta_2\\
\Leftrightarrow 1 = \theta_1 &\wedge \frac{5}{7} = \theta_2
\end{align*}
So we have three possible solutions for $\hat{\theta} \in [0,1]$:
\begin{align*}
\hat{\theta_0} = 0 \\
\hat{\theta_1} = 1 \\
\hat{\theta_2} = \frac{5}{7}
\end{align*}
Now we check more derivatives to see which is the local maximum that we are looking for:
\begin{align*}
f''(0) = f^3(0) = f^4(0) = 0 \not= f^5(0) &\Rightarrow \text{saddle point at }\hat{\theta_0}\\
f''(1) = 2 &\Rightarrow \text{minimum at }\hat{\theta_1}\\
f''(\frac{5}{7}) = -\frac{1250}{2401} &\Rightarrow \text{maximum at }\hat{\theta_2}
\end{align*}
So the maximum likelihood solution for $\hat{\theta}$ is $\frac{5}{7}$, which does make sense since it is the number of heads compared to the total number of tosses (which is the sample mean, if we assume a random variable $X$ with $X = 1$ if $x_k = head$ and $X = 0$ otherwise):
$$\hat{\theta} = \frac{\#\{ x = \text{head} \mid x \in D\}}{\#(D)} = \frac{5}{7}$$
With this and the fact that each sample $x_k$ is generated independently we can compute $P(x_8 = \text{head} ,\quad x_9 = \text{head} \mid \hat{\theta})$:
\begin{align*}
P(x_8 = \text{head} ,\quad x_9 = \text{head} \mid \hat{\theta}) = P(x_8 = \text{head} \mid \hat{\theta})P(x_9 = \text{head} \mid \hat{\theta}) = \hat{\theta}^2 = \frac{25}{49} \approx 0.51
\end{align*}
\item
First we want to compute the posterior destribution $p(\theta \mid \D)$. With our prior distribution
\begin{align*}
p(\theta) =
    \begin{cases}
        1    & \text{if } 0\le \theta \le 1\\
        0    & \text{else}
    \end{cases}
\end{align*}
we get:
\begin{align*}
    p(\theta \mid \D) &= \frac{p(\D \mid \theta)p(\theta)}{\int p(\D \mid \theta)p(\theta) d\theta} \\
    &= \alpha \prod_{k = 1}^{7} P(x_k \mid \theta) p(\theta)\\
    &=
    \begin{cases}
        \alpha \cdot \theta^5 \cdot (1 - \theta)^2    &\text{if } 0\le \theta \le 1\\0    & \text{else}
    \end{cases}        
\end{align*}
with:
\begin{align*}
    \alpha &= \frac{1}{\int \limits_{0}^1 \prod_{k = 1}^{7} P(x_k \mid \theta) p(\theta) d\theta}\\
    &= \frac{1}{\int \limits_{0}^1 \prod_{k = 1}^{7} P(x_k \mid \theta) d\theta}\\
    &= \frac{1}{\int \limits_{0}^1  \theta^5 \cdot (1 - \theta)^2 d\theta}\\
    &= \frac{1}{\int \limits_{0}^1 \theta^7 - 2\theta^6 + \theta^5 d\theta}\\
    &= \frac{1}{[\frac{\theta^8}{8}-\frac{2\cdot\theta^7}{7}+\frac{\theta^6}{6}]_{0}^1}\\
    &= \frac{1}{\frac{28}{168}-\frac{48}{168}+\frac{21}{168}}\\   
    &= \frac{1}{\frac{1}{168}} = 168
\end{align*}
Now we can evaluate the probability that the next two tosses are head:
\begin{align*}
\int P(x_8 = \text{head}, x_9 = \text{head} \mid \theta) p(\theta \mid D) d\theta &= \int P(x_8 = \text{head} \mid \theta) P(x_9 = \text{head} \mid \theta) p(\theta \mid D) d\theta\\
&= \int \limits_{0}^1 \theta^2 \cdot \alpha \cdot \theta^5\cdot (1-\theta)^2 d\theta\\
&= 168 \int \limits_{0}^1 \theta^7 (1-\theta)^2 d\theta\\
&= 168 \int \limits_{0}^1 \theta^9 - 2\theta^8+\theta^7 d\theta\\
&= 168 \cdot [\frac{\theta^{10}}{10}-\frac{2\cdot\theta^9}{9}+\frac{\theta^8}{8}]_{0}^1\\
&= 168 \cdot \frac{1}{360} = \frac{7}{15} \approx 0.4\overline{6}
\end{align*}

\end{enumerate}

\subsection*{Exercise 2}
\begin{enumerate}[(a)]
\item
To show
\begin{align*}
\sigma_{n}^2 \le \min{(\frac{\sigma^2}{n},\sigma_{0}^2)}
\end{align*}
we will at first solve the formula from section 3.4.1 of Duda et al. for $\sigma_{n}^2$:
\begin{align*}
    \frac{1}{\sigma_{n}^2} &= \frac{n}{\sigma^2}+ \frac{1}{\sigma_{0}^2}\\
    \Leftrightarrow 1 &= \frac{\sigma_{0}^2n+\sigma^2}{\sigma^2\sigma_{0}^2} \cdot \sigma_{n}^2\\
    \Leftrightarrow \sigma_{n}^2 &= \frac{\sigma^2\sigma_{0}^2}{\sigma_{0}^2n+\sigma^2}
\end{align*}
Now we can show $\sigma_{n}^2 \le \min{(\frac{\sigma^2}{n},\sigma_{0}^2)}$. We assume $\frac{\sigma^2}{n} \le \sigma_{0}^2$:
\begin{align*}
    \sigma_{n}^2 &\le \min{(\frac{\sigma^2}{n},\sigma_{0}^2)} \\
    \Leftrightarrow \frac{\sigma^2\sigma_{0}^2}{\sigma_{0}^2n+\sigma^2} &\le \frac{\sigma^2}{n}\\
    \Leftrightarrow \sigma^2\sigma_{0}^2 &\le \sigma^2\sigma_{0}^2 + \frac{\sigma^4}{n}\\
    \Leftrightarrow \sigma^2\sigma_{0}^2 &\le \sigma^2 \cdot (\sigma_{0}^2 + \frac{\sigma^2}{n}\\
    \Leftrightarrow \sigma_{0}^2 &\le \sigma_{0}^2 + \frac{\sigma^2}{n}\\
    \Leftrightarrow 0 &\le \frac{\sigma^2}{n}
\end{align*}
This holds true since $n$ is the number of features and $\sigma^2$ is the variance of a Gaussian distribution $\mathcal{N}(\mu,\sigma^2)$, thus $n, \sigma > 0$.

Now we assume the other case ($\sigma_{0}^2 \le \frac{\sigma^2}{n}$):
\begin{align*}
    \sigma_{n}^2 &\le \min{(\frac{\sigma^2}{n},\sigma_{0}^2)} \\
    \Leftrightarrow \frac{\sigma^2\sigma_{0}^2}{\sigma_{0}^2n+\sigma^2} &\le \sigma_{0}^2\\
    \Leftrightarrow \sigma^2\sigma_{0}^2 &\le \sigma_{0}^2 \cdot (\sigma_{0}^2n+\sigma^2)\\
    \Leftrightarrow \sigma^2 &\le \sigma_{0}^2n+\sigma^2\\
    \Leftrightarrow 0 &\le \sigma_{0}^2n
\end{align*}
This again holds true since $n > 0$ as stated above and $\sigma_{0}^2 > 0$ since it is the variance of the Gaussian distribution $\mathcal{N}(\mu_0,\sigma_{0}^2)$
\item
To show
\begin{align*}
    \min{(\mu_0,\hat{\mu}_n)} \le \mu_n \le \max{(\mu_0,\hat{\mu}_n)}
\end{align*}
we again first solve the formula from section 3.4.1 of Duda et al. for $\mu_n$, also using our $\sigma_{n}^2$ from above:
\begin{align*}
    \frac{\mu_n}{\sigma_{n}^2} &= \frac{n}{\sigma^2}\hat{\mu}_n+\frac{\mu_0}{\sigma_{0}^2}\\
    \Leftrightarrow \mu_n &= (\frac{n}{\sigma^2}\hat{\mu}_n+\frac{\mu_0}{\sigma_{0}^2}) \cdot \sigma_{0}^2\\
    \Leftrightarrow \mu_n &= (\frac{n}{\sigma^2}\hat{\mu}_n+\frac{\mu_0}{\sigma_{0}^2}) \cdot \frac{\sigma^2\sigma_{0}^2}{\sigma_{0}^2n+\sigma^2}\\
    \Leftrightarrow \mu_n &= \frac{n\sigma_{0}^2}{n\sigma_{0}^2+\sigma^2}\hat{\mu}_n+\frac{\sigma^2}{n\sigma_{0}^2+\sigma^2}\mu_0
\end{align*}
Lets first assume $\mu_0 < \hat{\mu}_n$ (1) and show the first part of the inequality:
\begin{align*}
    \min{(\mu_0,\hat{\mu}_n)} &\le \mu_n \\
    \Leftrightarrow \mu_0 &\le \mu_n\\
    \Leftrightarrow \mu_0 &\le \frac{n\sigma_{0}^2}{n\sigma_{0}^2+\sigma^2}\hat{\mu}_n+\frac{\sigma^2}{n\sigma_{0}^2+\sigma^2}\mu_0\\
    \Leftrightarrow \mu_0 \cdot (n\sigma_{0}^2+\sigma^2) &\le n\sigma_{0}^2\hat{\mu}_n + \sigma^2\mu_0\\
    \Leftrightarrow \mu_0 n\sigma_{0}^2 &\le n\sigma_{0}^2\hat{\mu}_n\\
    \Leftrightarrow \mu_0 &\le \hat{\mu}_n
\end{align*}
This hold true due to our assumption (1) from above. Let's show the second part of the inequality:
\begin{align*}
    \mu_n &\le \max{(\mu_0,\hat{\mu}_n)}\\
    \Leftrightarrow \mu_n &\le \hat{\mu}_n\\
    \Leftrightarrow \frac{n\sigma_{0}^2}{n\sigma_{0}^2+\sigma^2}\hat{\mu}_n+\frac{\sigma^2}{n\sigma_{0}^2+\sigma^2}\mu_0 &\le \hat{\mu}_n\\
    \Leftrightarrow n\sigma_{0}^2\hat{\mu}_n+\sigma^2\mu_0 &\le \hat{\mu}_n \cdot (n\sigma_{0}^2+\sigma^2)\\
    \Leftrightarrow \sigma^2\mu_0 &\le \hat{\mu}_n\sigma^2\\
    \Leftrightarrow \mu_0 &\le \hat{\mu}_n
\end{align*}
This again holds true due to our assumption (1) from above.

Now we'll assume that $\hat{\mu}_n < \mu_0$ (2) and show that our inequality still holds. We'll start with the first part again:
\begin{align*}
    \min{(\mu_0,\hat{\mu}_n)} &\le \mu_n \\
    \Leftrightarrow \hat{\mu}_n &\le \mu_n\\
    \Leftrightarrow \hat{\mu}_n &\le \frac{n\sigma_{0}^2}{n\sigma_{0}^2+\sigma^2}\hat{\mu}_n+\frac{\sigma^2}{n\sigma_{0}^2+\sigma^2}\mu_0\\
    \Leftrightarrow \hat{\mu}_n \cdot (n\sigma_{0}^2+\sigma^2) &\le n\sigma_{0}^2\hat{\mu}_n + \sigma^2\mu_0\\
    \Leftrightarrow \hat{\mu}_n \sigma^2 &\le \sigma^2\mu_0\\   
    \Leftrightarrow \hat{\mu}_n &\le \mu_0
\end{align*}
This hold true due to our assumption (2) from above. Let's show the second part of the inequality:
\begin{align*}
    \mu_n &\le \max{(\mu_0,\hat{\mu}_n)}\\
    \Leftrightarrow \mu_n &\le \mu_0\\
    \Leftrightarrow \frac{n\sigma_{0}^2}{n\sigma_{0}^2+\sigma^2}\hat{\mu}_n+\frac{\sigma^2}{n\sigma_{0}^2+\sigma^2}\mu_0 &\le \mu_0\\
     \Leftrightarrow n\sigma_{0}^2\hat{\mu}_n+\sigma^2\mu_0 &\le \mu_0 \cdot (n\sigma_{0}^2+\sigma^2)\\
     \Leftrightarrow n\sigma_{0}^2\hat{\mu}_n &\le \mu_0n\sigma_{0}^2\\
     \Leftrightarrow \hat{\mu}_n &\le \mu_0
\end{align*}
This again holds true due to our assumption (2) from above.
\end{enumerate}


\end{document}