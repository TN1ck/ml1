{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1> Machine Learning I </h1>\n",
      "\n",
      "<h3>Maximum Likelihood </h3>\n",
      "\n",
      "In this assignment we will improve our naive Bayes classifier by including information about the number of occurences of interesting words. Instead of using Bernuli distributions to model word occurence (1 - contained in text, 0 - missing from text), we will try using three different discrete distributions to model the conditional  $p(\\# word \\; occurences | class)$ distributions. We will use maximum likelihood estimation to determine the parameters of these distributions and later classify our points by checking if\n",
      "\n",
      "$p(class = spam | message ) > p(class = ham | message)$\n",
      "\n",
      "To check this, we will compare\n",
      "\n",
      "$p(class = spam)\\prod_{word \\in Dictionary} p(count = word count | class=spam) $ and $ p(class = ham)\\prod_{word \\in Dictionary} p(count = word count | class=ham) $\n",
      "\n",
      "where the dictionary is the set of all words we are examining and won't be altered during training i.e. our column parameter is the whole set.\n",
      "\n",
      "Our data, taken from http://archive.ics.uci.edu/ml/datasets/Spambase will consist of 56 columns where:\n",
      "\n",
      "- The first 48 measure the number of occurences of specific words\n",
      "\n",
      "- The following 6 measure frequencies of specific characters (not specified in the source)\n",
      "\n",
      "- The following 3 gather statistics of capital letter sequences, a common indicator of spam messages\n",
      "\n",
      "We will first examine how well the three distributions fit our data. The necessary parameters for the data will be estimated using maximum likelihood estimation.\n",
      "\n",
      "The three distributions we'll be using are:\n",
      "\n",
      "- Binomial distribution. Information about it can be found here http://en.wikipedia.org/wiki/Binomial_distribution . \n",
      "    - The number of successes (k) is the value of the current cell.\n",
      "    - p should be estimated from the training data for each class\n",
      "    - The number of draws will be set to 20 for columns 0-53 and 200 for the last three columns. \n",
      "- Geometric distribution. Information about it can be found here http://en.wikipedia.org/wiki/Geometric_distribution Note that we use the second type of notation from the page (our values start with 0)\n",
      "    - k = the value of the cell\n",
      "    - p should be estimated from the training data for each class\n",
      "- Poisson distribution. Information about it can be found here http://en.wikipedia.org/wiki/Poisson_distribution .\n",
      "    - k is the value of the cell\n",
      "    - $\\lambda$ should be estimated from the training data for each class"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import csv\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import special\n",
      "%pylab inline\n",
      "\n",
      "def getData(number_of_partitions,desired_test_partition):\n",
      "    data = np.genfromtxt('data.csv', delimiter=',') # Read the csv\n",
      "    \n",
      "    random.seed(31415) # Set a seed for the random generator\n",
      "    \n",
      "    hamData=data[data[:,57]==0,0:56] # Split the ham and spam data\n",
      "    spamData=data[data[:,57]==1,0:56] # Split the ham and spam data\n",
      "    data = 0\n",
      "    \n",
      "    np.random.shuffle(hamData) # Shuffle around (using the set seed)\n",
      "    np.random.shuffle(spamData)\n",
      "    \n",
      "    #Determine the range of ham messages used for testing\n",
      "    hamStartIndex=int(desired_test_partition*hamData.shape[0]/number_of_partitions)\n",
      "    hamStopIndex=int(hamStartIndex+hamData.shape[0]/number_of_partitions)\n",
      "    \n",
      "    #Determine the range of spam messages used for testing\n",
      "    spamStartIndex=int(desired_test_partition*spamData.shape[0]/number_of_partitions)\n",
      "    spamStopIndex=int(spamStartIndex+spamData.shape[0]/number_of_partitions)\n",
      "    \n",
      "    #Split the sets\n",
      "    hamTest = hamData[hamStartIndex:hamStopIndex,:]\n",
      "    hamTrain=np.delete(hamData,range(hamStartIndex,hamStopIndex),axis=0)\n",
      "    spamTest=spamData[spamStartIndex:spamStopIndex,:]\n",
      "    spamTrain=np.delete(spamData,range(spamStartIndex,spamStopIndex),axis=0)\n",
      "    return hamTrain,hamTest,spamTrain,spamTest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hamTrain,hamTest,spamTrain,spamTest = getData(4,2)\n",
      "#optional, in case you prefer your data to be together when testing\n",
      "testData = np.vstack([hamTest,spamTest])\n",
      "testLabels = np.zeros((hamTest.shape[0]+spamTest.shape[0],1))\n",
      "testLabels[hamTest.shape[0]:]=1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "hamTrain and spamTrain contain are [number_of_points x number_of_features] matrices containing the data used for parameter estimation. You can use the test data either split into hamTest and spamTest (separated by class) or as testData, testLabels"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Part I: Estimating class frequencies (5P)</h3>\n",
      "\n",
      "We assume that the priors for message class follow a Bernoulli distribution:\n",
      "\n",
      "$p(class=spam) = \\theta$ and $p(class=ham) = 1-\\theta$. \n",
      "\n",
      "Estimate the parameter $\\theta$ from the training data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#TODO: Estimate the priors\n",
      "def compute_class_theta(spam=spamTrain, ham=hamTrain):\n",
      "    # The maximum likelhood for a Bernoulli distribution is simply the class mean\n",
      "    sigma = float(spam.shape[0])/(spam.shape[0] + ham.shape[0])\n",
      "    return sigma\n",
      "    \n",
      "theta = compute_class_sigma()\n",
      "theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 72,
       "text": [
        "0.39408866995073893"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3> Part II: Estimating parameters of the data-generating distributions (10P)</h3>\n",
      "\n",
      "First will first estimate the parameters for the distributions using maximum likelihood. Determine the parameters for each of the distributions. They are:\n",
      "\n",
      "- p for the binomial distribution\n",
      "- p for the geometric distribution\n",
      "- $\\lambda$ for the poisson distribution\n",
      "\n",
      "then plot them, along with the data which they are supposed to model. Include the plots for columns 0-10, 47-51 and the last 4 columns. A helper function for plotting has been included. How you compute the values is entirely up to you."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "number_of_samples = testData.shape[0]\n",
      "\n",
      "def count_occurence(bin_size, samples):\n",
      "    counts, bins = np.histogram(samples, bins=range(0,bin_size + 1,1), density=False)\n",
      "    return counts\n",
      "\n",
      "def bin_theta(index, data, N):\n",
      "    column = data[:,index]\n",
      "    mean = np.mean(column)\n",
      "    result = mean/N\n",
      "    return result\n",
      "\n",
      "def bin_distribution(N, sigma, samples):\n",
      "    # draw as many as we have data points\n",
      "    bin_samples = numpy.random.binomial(N, sigma, samples)\n",
      "    counts = count_occurence(N, bin_samples)\n",
      "    return counts\n",
      "\n",
      "def geo_distribution(N, sigma, samples):\n",
      "    # we model the number of failures, so subtract 1\n",
      "    geo_samples = numpy.subtract(numpy.random.geometric(sigma, samples), numpy.ones(samples))\n",
      "    counts = count_occurence(N, geo_samples)\n",
      "    return counts\n",
      "\n",
      "def geo_theta(index, data):\n",
      "    column = data[:,index]\n",
      "    result = column.shape[0]/(np.sum(column) + column.shape[0])\n",
      "    return result\n",
      "\n",
      "def poisson_distribution(N, sigma, samples):\n",
      "    poisson_samples = numpy.random.poisson(sigma, samples)\n",
      "    counts = count_occurence(N, poisson_samples)\n",
      "    return counts\n",
      "\n",
      "def poisson_theta(index, data):\n",
      "    column = data[:,index]\n",
      "    return np.mean(column)\n",
      "\n",
      "def show_estimates(index, data):\n",
      "    \"\"\"\n",
      "        Plots the estimated distribution alongside the data itself the given column\n",
      "        \n",
      "        index - The column we are currently plotting for\n",
      "        data - The training data to be used.\n",
      "    \"\"\"\n",
      "    if(index<53):\n",
      "        N=20\n",
      "    else:\n",
      "        N=200\n",
      "    #This is what we used, feel free to change anything you like    \n",
      "        \n",
      "    plt.figure(figsize=(10,3))\n",
      "        \n",
      "    #Determine the values for the binomial distribution on range(0,N)\n",
      "    \n",
      "    bin_Y = bin_distribution(N, bin_theta(index, data, N), data.shape[0])\n",
      "    ax1=plt.subplot(1,3,1)\n",
      "    ax1.plot(np.arange(0.5,N+0.5,1.),bin_Y,'x-') #The binomial distribution you've estimated\n",
      "    ax1.hist(data[:,index], bins=range(0,N,1), color='red',alpha=0.5, label='actual') #The data itself\n",
      "    ax1.set_title('Binomial col =%d'%index)\n",
      "    \n",
      "    plt.subplot(1,3,2)\n",
      "    #Determine the values of the geometric distribution on range(0,N)\n",
      "    \n",
      "    geo_Y = geo_distribution(N, geo_theta(index, data), data.shape[0])\n",
      "    plt.plot(np.arange(0.5,N+0.5,1.),geo_Y,'x-')\n",
      "    plt.hist(data[:,index], bins=range(0,N,1), color='red',alpha=0.5, label='actual')\n",
      "    plt.title('Geometrical col =%d'%index)\n",
      "    \n",
      "    plt.subplot(1,3,3)\n",
      "    ##Determine the values of the poisson distribution on range(0,N)\n",
      "    \n",
      "    poisson_Y = poisson_distribution(N, poisson_theta(index, data), data.shape[0])\n",
      "    plt.plot(np.arange(0.5,N+0.5,1.),poisson_Y,'x-')\n",
      "    plt.hist(data[:,index], bins=range(0,N,1), color='red',alpha=0.5, label='actual')\n",
      "    plt.title('Poisson col =%d'%index)\n",
      "    plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And the plots you asked for:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#for i in (range(0, 11) + range(47, 56)):\n",
      "#    show_estimates(i, spamTrain)\n",
      "#    show_estimates(i, hamTrain)\n",
      "from scipy.stats import binom\n",
      "binom.pmf(spamTrain[:,0], 5, bin_sigma(0, spamTrain, 20), loc=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 88,
       "text": [
        "array([ 0.96301549,  0.        ,  0.        , ...,  0.        ,\n",
        "        0.96301549,  0.96301549])"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Part III: Applying a Bayes Classifier (15P)</h3>\n",
      "\n",
      "Using all the parameters that you have estimated in Part I, implement the simple Bayes classifier. To achieve this, you need to compute the (unnormalized) class posteriors:\n",
      "\n",
      "$p(class = spam)\\prod_{j} p(count = data[i,j] | class=spam) $ and $ p(class = ham)\\prod_{j} p(count = data[i,j] | class=ham) $\n",
      "\n",
      "for every row $i$ of your data matrix, and choose the class based on the Bayes decision rule.\n",
      "\n",
      "Use the priors estimated in part I."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import stats\n",
      "from math import isnan\n",
      "\n",
      "def classify():\n",
      "    p_spam = compute_class_sigma()\n",
      "    p_ham = 1 - p_spam\n",
      "    # Pep8 :)\n",
      "    spam_test = spamTest\n",
      "    ham_test = hamTest\n",
      "    spam_train = spamTrain\n",
      "    ham_train = hamTrain\n",
      "    \n",
      "    # estimate the paremeters using the maximum-likelihood methods from Part II\n",
      "    # we're using the training-data for this\n",
      "    def compute_parameter_estimations(data):\n",
      "        mass_functions = []\n",
      "        for i in range(0, data.shape[1]):\n",
      "            column = data[:, i]\n",
      "            N = 50 if i < 53 else 200\n",
      "            \n",
      "            x = range(0, N)\n",
      "            \n",
      "            bino = stats.binom.pmf(x, N, bin_theta(i, data, N))\n",
      "            geo = stats.geom.pmf(range(1, N + 1), geo_theta(i, data))\n",
      "            poisson = stats.poisson.pmf(x, poisson_theta(i, data))\n",
      "            mass_functions.append((bino, geo, poisson,))\n",
      "        return mass_functions\n",
      "    \n",
      "    spam_pmfs = compute_parameter_estimations(spam_train)\n",
      "    ham_pmfs = compute_parameter_estimations(ham_train)\n",
      "    \n",
      "    def compute_posteriors(data, pmfs, p_class):\n",
      "        class_posteriors = []\n",
      "        for row in data:\n",
      "            class_posterior = [p_class,p_class,p_class]\n",
      "            for index in range(0,47):\n",
      "                word_count = row[index]\n",
      "                bino, geo, poisson = pmfs[index]\n",
      "                bino_pmf = bino[int(word_count)] or 1\n",
      "                geo_pmf = geo[int(word_count)] or 1\n",
      "                poisson_pmf = poisson[int(word_count)] or 1\n",
      "                class_posterior[0] *= 1 if isnan(bino_pmf) else bino_pmf\n",
      "                class_posterior[1] *= 1 if isnan(geo_pmf) else geo_pmf\n",
      "                class_posterior[2] *= 1 if isnan(poisson_pmf) else poisson_pmf\n",
      "            class_posteriors.append(class_posterior)\n",
      "        return numpy.array(class_posteriors)\n",
      "    \n",
      "    # there are 4 combinations possible\n",
      "    \n",
      "    # * use spam_test assuming it is spam\n",
      "    spam_spam_posteriors = compute_posteriors(spam_test, spam_pmfs, p_spam)\n",
      "    # * use spam_test assuming it is ham\n",
      "    spam_ham_posteriors = compute_posteriors(spam_test, ham_pmfs, p_ham)\n",
      "    # * use ham_test assuming it is ham\n",
      "    ham_ham_posteriors = compute_posteriors(ham_test, ham_pmfs, p_ham)\n",
      "    # * use ham_test assuming it is spam\n",
      "    ham_spam_posteriors = compute_posteriors(ham_test, spam_pmfs, p_spam)\n",
      "    \n",
      "    # for every distribution, classify the data-points\n",
      "    def compare(correct_class, false_class):\n",
      "        results = []\n",
      "        for i, name in zip(range(0,3), ['binomial', 'poisson', 'geometric']):\n",
      "            posterior_1 = correct_class[:,i]\n",
      "            posterior_2 = false_class[:,i]\n",
      "            results.append([numpy.greater_equal(posterior_1, posterior_2), name])\n",
      "        return results\n",
      "    \n",
      "    def print_result(results, c):\n",
      "        for result, name in results:\n",
      "            correct = float(numpy.sum(result))/float(result.shape[0])\n",
      "            print 'correct classified %s using the %s distribution: %f' % (c, name, correct)\n",
      "    \n",
      "    correct_spam = compare(spam_spam_posteriors, spam_ham_posteriors)\n",
      "    correct_ham = compare(ham_ham_posteriors, ham_spam_posteriors)\n",
      "    \n",
      "    print_result(correct_spam, 'spam')\n",
      "    print_result(correct_ham, 'ham')\n",
      "    \n",
      "classify()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "correct classified spam using the binomial distribution: 0.876380\n",
        "correct classified spam using the poisson distribution: 0.871965\n",
        "correct classified spam using the geometric distribution: 0.876380\n",
        "correct classified ham using the binomial distribution: 0.796270\n",
        "correct classified ham using the poisson distribution: 0.802009\n",
        "correct classified ham using the geometric distribution: 0.797704\n"
       ]
      }
     ],
     "prompt_number": 168
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Answer the following questions:\n",
      "\n",
      "- How did each of the classifiers perform. Was this expected?\n",
      "    - Answer here\n",
      "- Why does the estimated distribution for the last column vary so much from the actual distribution?\n",
      "    - The geometrical-disturbution matches it pretty well for the p we choose. The trivial explanation would be, that selected models just don't fit the data and it's distribution function is something different. The big random spikes also indicate that the data is quite noisy.\n",
      "- Suggest a way to combine everything you've done above and build a better classifier. You may add additional parameters and training stages.\n",
      "    - Answer here"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}