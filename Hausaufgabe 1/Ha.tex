\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\setlength{\parindent}{0cm}
\usepackage{setspace}
\usepackage{mathpazo}
\usepackage{graphicx}
\usepackage{wasysym} 
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{microtype}
\usepackage{siunitx}
\usepackage{cleveref}
\usepackage[colorlinks=false, pdfborder={0 0 0}]{hyperref}
\usepackage{paralist}
\usepackage{pst-all}
\usepackage{pstricks}
\usepackage{pst-node}
\usepackage{tikz}
\usepackage{tkz-berge}
\usetikzlibrary{trees,petri,decorations,arrows,automata,shapes,shadows,positioning,plotmarks}
\usepackage[a4paper,
left=3.0cm, right=3.0cm,
top=2.0cm, bottom=2.0cm]{geometry}
\usepackage{fullpage}
\usepackage[german]{babel}
%\usepackage{pst-all}
%\usepackage{pstricks}
\setlength{\unitlength}{1cm}
\newcommand{\N}{\mathbb{N}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\R}{\mathbb{R}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\usepackage{mathpazo}

\author{Tom Nick, Niklas Gebauer, Felix Bohmann}
\title{Machine Learning}
\begin{document}
\begin{center}
\Large{\textsc{Machine Learning 1: Assignment 1}} \\
\end{center}

\begin{tabbing}
Tom Nick \hspace{0.9cm}\= 340528\\
Niklas Gebauer\>  xxxxx \\
Felix Bohmann\> xxxxxx
\end{tabbing}

\subsection*{Exercise 1}
\subsubsection*{a)}
\begin{align}
	P(error) &= \int P(error\mid x)p(x) dx \label{eq1} \\
	P(error\mid x) &= \min (P(w_1\mid x), P(w_2\mid x)) \label{eq2}
\end{align}

With these equations, we want to show that

\begin{align}
	P(error) \le \int \frac{2}{\frac{1}{P(w_1\mid x)} + \frac{1}{P(w_2 \mid x)}} p(x) dx \label{eq3}
\end{align}

At first, without restricting the general case, we assume that $P(w_1 \mid x) \ge P(w_2 \mid x) \label{ass1}$, that is the function $P(error \mid x) = P(w_2 \mid x)$. Now with \eqref{eq1}, \eqref{eq2} and \eqref{eq3} we have:

\begin{align*}
	\int P(w_2 \mid x)p(x) dx \le \int \frac{2}{\frac{1}{P(w_1\mid x)} + \frac{1}{P(w_2 \mid x)}} p(x) dx
\end{align*}

Because both sides are integrating over the same variable we can simplify the term to:
%\begin{align*}
%	&P(w_2 \mid x)p(x) \le \frac{2}{\frac{1}{P(w_1\mid x)} + \frac{1}{P(w_2 \mid x)}} p(x) \\
%	&\Leftrightarrow \frac{1}{P(w_2 \mid x)p(x)} \ge \frac{\frac{1}{P(w_1\mid x)} + \frac{1}{P(w_2 \mid x)}}{2} p(x) \\
%	&\Leftrightarrow \frac{1}{P(w_2 \mid x)} \ge \frac{\frac{1}{P(w_1\mid x)} + \frac{1}{P(w_2 \mid x)}}{2} \\
%	&\Leftrightarrow \frac{2}{P(w_2 \mid x)} \ge \frac{1}{P(w_1\mid x)} + \frac{1}{P(w_2 \mid x)}  \\
%	&\Leftrightarrow \frac{1}{P(w_2 \mid x)} \ge \frac{1}{P(w_1\mid x)} \\
%	&\Leftrightarrow P(w_2 \mid x) \le P(w_1\mid x)

\begin{align*}	
	&P(w_2 \mid x)p(x) \le \frac{2}{\frac{1}{P(w_1\mid x)} + \frac{1}{P(w_2 \mid x)}} p(x) \\
	&\Leftrightarrow P(w_2 \mid x) \le \frac{2}{\frac{1}{P(w_1\mid x)} + \frac{1}{P(w_2 \mid x)}} \\
	&\Leftrightarrow (\frac{1}{P(w_1\mid x)} + \frac{1}{P(w_2 \mid x)}) P(w_2 \mid x) \le 2 \\
	&\Leftrightarrow \frac{1}{P(w_1\mid x)} + \frac{1}{P(w_2 \mid x)} \le \frac{2}{P(w_2 \mid x)} \\
	&\Leftrightarrow \frac{1}{P(w_1 \mid x)} \le \frac{1}{P(w_2\mid x)} \\
	&\Leftrightarrow P(w_1 \mid x) \ge P(w_2\mid x)
\end{align*}

This holds true with the assumptions \ref{ass1} we made earlier.

\subsubsection*{b)}

With this result, we now show that:

\begin{align*}
P(error) \le \frac{2P(w_1)P(w_2)}{\sqrt{P(w_1)^2 + (4\mu^2 + 2)P(w_1)P(w_2) + P(w_2)^2}}
\end{align*}

While using the univariate probability distribution:
$$p(x\mid w_1) = \frac{\pi^{-1}}{1+ (x - \mu)^2} \textrm{ and } p(x \mid w_2) = \frac{\pi^{-1}}{1 + (x + \mu)^2}$$

With the rule of bayes we have $P(w_1 \mid x) = \frac{p(x \mid w_1)P(w_1)}{p(x)}$:

\begin{align*}
	P(error) &\le \int \frac{2}{\frac{1}{P(w_1\mid x)} + \frac{1}{P(w_2 \mid x)}} p(x) dx \\
	\Leftrightarrow P(error) &\le \int \frac{2}{\frac{1}{ \frac{p(x \mid w_1)P(w_1)}{p(x)} } + \frac{1}{\frac{p(x \mid w_2)P(w_2)}{p(x)}}} p(x) dx \\
	\Leftrightarrow P(error) &\le \int \frac{2}{\frac{p(x)}{p(x \mid w_1)P(w_1)} + \frac{p(x)}{p(x \mid w_2)P(w_2)}} p(x) dx \\
	\Leftrightarrow P(error) &\le \int \frac{2}{\frac{1}{p(x \mid w_1)P(w_1)} + \frac{1}{p(x \mid w_2)P(w_2)}}dx \\
	\Leftrightarrow P(error) &\le \int \frac{2}{\frac{1}{ \frac{\pi^{-1}}{1+ (x - \mu)^2} P(w_1)} + \frac{1}{ \frac{\pi^{-1}}{1 + (x + \mu)^2} P(w_2)}}dx \\
	\Leftrightarrow P(error) &\le \int \frac{2}{\frac{1}{ \frac{\pi^{-1} P(w_1)}{1+ (x - \mu)^2}} + \frac{1}{ \frac{\pi^{-1} P(w_2)}{1 + (x + \mu)^2}}}dx \\
	\Leftrightarrow P(error) &\le \int \frac{2}{\frac{1+ (x - \mu)^2}{\pi^{-1} P(w_1)} +  \frac{1 + (x + \mu)^2}{\pi^{-1} P(w_2)}}dx \\
		\Leftrightarrow P(error) &\le \int \frac{2}{\frac{(1+ (x - \mu)^2) P(w_2)}{\pi^{-1} P(w_1) P(w_2)} +  \frac{(1 + (x + \mu)^2) P(w_1)}{\pi^{-1} P(w_2) P(w_1)}}dx \\
		\Leftrightarrow P(error) &\le \int \frac{2 \pi^{-1} P(w_2) P(w_1)}{(1+ (x + \mu)^2) P(w_1) + (1 + (x - \mu)^2) P(w_2)}dx \\
		\Leftrightarrow P(error) &\le \int \frac{2 \pi^{-1} P(w_2) P(w_1)}{(x^2 + 2x\mu +\mu^2 +1) P(w_1) + (x^2 - 2x\mu +\mu^2 +1) P(w_2)}dx \\
		\Leftrightarrow P(error) &\le \int \frac{2 \pi^{-1} P(w_2) P(w_1)}{(P(w_1)+P(w_2))x^2 + (P(w_1)-P(w_2))2\mu x + (P(w_1)\mu^2 + P(w_2)\mu^2+P(w1)+P(w_2))}dx \\
\end{align*}

We can now take out the numerator of the integral and use the following equation:

\begin{align}
\int \frac{1}{ax^2+bx+c}dx = \frac{2\pi}{\sqrt{4ac-b^2}}
\end{align}

with:

\begin{align*}
a &= P(w_1)+P(w_2) \\
b &= (P(w_1)-P(w_2))2\mu \\
c &= P(w_1)\mu^2 + P(w_2)\mu^2+P(w1)+P(w_2) \\
\end{align*}

because

\begin{align*}
    b^2 &< 4ac \\
    \Leftrightarrow 0 &< 4ac-b^2 \\
    \Leftrightarrow 0 &< 4(P(w_1)+P(w_2))(P(w_1)\mu^2 + P(w_2)\mu^2+P(w1)+P(w_2))-((P(w_1)-P(w_2))2\mu)^2 \\
    \Leftrightarrow 0 &< 4P(w_1)^2\mu^2 + 4P(w_1)P(w_2)\mu^2+4P(w_1)^2+4P(w_1)P(w_2) \\
    &+ 4P(w_1)P(w_2)\mu^2+4P(w_2)^2\mu^2+ 4P(w_2)P(w_1)+4P(w_2)^2 \\
    &- (4P(w_1)^2\mu^2-8P(w_1)P(w_2)\mu^2+4P(w_2)^2\mu^2) \\
    \Leftrightarrow 0 &< 16P(w_1)P(w_2)\mu^2+8P(w_1)P(w_2)+4P(w_1)^2+4P(w_2)^2 \\
\end{align*}

and this holds since $P(w_1), P(w_2) \in [0,1]$ and $P(w_1) + P(w_2) = 1$.\\
We now already calculated $4ac-b^2$ in the steps before and just need to use $(4)$ to proceed where we stopped before introducing equation 4:

\begin{align*}
    \Leftrightarrow P(error) &\le \int \frac{2 \pi^{-1} P(w_2) P(w_1)}{(P(w_1)+P(w_2))x^2 + (P(w_1)-P(w_2))2\mu x + (P(w_1)\mu^2 + P(w_2)\mu^2+P(w1)+P(w_2))}dx \\
    \Leftrightarrow P(error) &\le 2 \pi^{-1} P(w_2) P(w_1) \frac{2\pi}{\sqrt{16P(w_1)P(w_2)\mu^2+8P(w_1)P(w_2)+4P(w_1)^2+4P(w_2)^2}} \\
    \Leftrightarrow P(error) &\le \frac{4 P(w_1) P(w_2) }{\sqrt{4((4\mu^2+2)P(w_1)P(w_2)+P(w_1)^2+P(w_2)^2)}} \\
    \Leftrightarrow P(error) &\le \frac{4 P(w_1) P(w_2) }{\sqrt{4}\sqrt{((4\mu^2+2)P(w_1)P(w_2)+P(w_1)^2+P(w_2)^2)}} \\
    \Leftrightarrow P(error) &\le \frac{2 P(w_1) P(w_2) }{\sqrt{P(w_1)^2+(4\mu^2+2)P(w_1)P(w_2)+P(w_2)^2}} \\
\end{align*}

\subsubsection*{c)}
According to informations form chapter 2.8 in Pattern Classification:

For both case we can find the upper-bound numericaly via the Chernoff Bound or the Bhattacharyya Bound, this may lead to a tighter upper bound than the analytical version, which is even harder to approximate due to the discontinues nature of the integrals. For the high-dimensional space the Bhattacharyya Bound might be better, because it is computationally less expensive that the Chernoff Bound.

\subsection*{Exercise 2}
\subsubsection*{a)}
The data is generated by the univariate Laplacian Distrubution:
$$p(x\mid w_1) = \frac{1}{2\sigma}\exp\left(-\frac{\mid x - \mu \mid}{\sigma}\right) \textrm{ and } p(x\mid w_2) = \frac{1}{2\sigma}\exp\left(-\frac{\mid x + \mu \mid}{\sigma}\right)$$
To get the optimal decision boundary we have to solve $P(w_1\mid x) = P(w_2 \mid x)$ for x.
\begin{align*}
&& P(w_1 \mid x) &= P(w_2 \mid x) \\
&\Leftrightarrow &\frac{p(x \mid w_1)P(w_1)}{p(x)} &= \frac{p(x \mid w_2)P(w_2)}{p(x)} \\
&\Leftrightarrow &p(x\mid w_1)P(w_1) &= p(x \mid w_2)P(w_2) \\
&\Leftrightarrow &\frac{1}{2\sigma}\exp\left(-\frac{\mid x - \mu \mid}{\sigma}\right)P(w_1) &= \frac{1}{2\sigma}\exp\left(-\frac{\mid x + \mu \mid}{\sigma}\right)P(w_2) \\
&\Leftrightarrow &\exp\left(-\frac{\mid x - \mu \mid}{\sigma}\right)P(w_1) &= \exp\left(-\frac{\mid x + \mu \mid}{\sigma}\right)P(w_2) \\
&\Leftrightarrow &\ln(\exp\left(-\frac{\mid x - \mu \mid}{\sigma}\right)P(w_1)) &= \ln(\exp\left(-\frac{\mid x + \mu \mid}{\sigma}\right)P(w_2)) \\
&\Leftrightarrow &-\frac{\mid x - \mu \mid}{\sigma} + \ln(P(w_1)) &= -\frac{\mid x + \mu \mid}{\sigma} + \ln(P(w_2)) \\
&\Leftrightarrow & \ln(P(w_1)) - \ln(P(w_2)) &= \frac{\mid x - \mu \mid}{\sigma} - \frac{\mid x + \mu \mid}{\sigma} \\
&\Leftrightarrow & \ln\left(\frac{P(w_1)}{P(w_2)}\right) &= \frac{\mid x - \mu \mid - \mid x + \mu \mid}{\sigma} \\
&\Leftrightarrow & \sigma\ln\left(\frac{P(w_1)}{P(w_2)}\right) &= \mid x - \mu \mid - \mid x + \mu \mid
\end{align*}
There are now 3 cases to look at:
\begin{enumerate}
	\item $x \le \mu$ and $-x \le \mu$ \\
		\begin{align*}
			\sigma\ln\left(\frac{P(w_1)}{P(w_2)}\right) &= -(x - \mu) - (x + \mu ) \\
			&= 2(\mu - x)
		\end{align*}
	\item $x \le \mu$ and $-x > \mu$ \\
		\begin{align*}
			\sigma\ln\left(\frac{P(w_1)}{P(w_2)}\right) &= -(x - \mu) + (x + \mu ) \\
			&= 0 \Rightarrow P(w_1) = P(w_2)
		\end{align*}
	\item $x > \mu$ \\
		\begin{align*}
			\sigma\ln\left(\frac{P(w_1)}{P(w_2)}\right) &= (x - \mu) - (x + \mu ) \\
			&= 2\mu
		\end{align*}
\end{enumerate}
\subsubsection*{b)}
To find out for which values of $P(w_1), P(w_2), \mu, \sigma$ the optimal decision is to always predict the first class, which means $P(error \mid x) = P(w_2 \mid x)$, so we have to show $P(w_2 \mid x) \le P(w_1 \mid x)$:







\end{document}